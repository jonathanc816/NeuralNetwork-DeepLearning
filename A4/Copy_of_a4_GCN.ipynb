{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLCXm8IsSS2"
      },
      "source": [
        "# Download the Cora data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xRN47p1SKRgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d3976a-5b41-4c81-ff40-a2dfc88653be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-07 03:07:21--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
            "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
            "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168052 (164K) [application/x-gzip]\n",
            "Saving to: ‘cora.tgz.2’\n",
            "\n",
            "cora.tgz.2          100%[===================>] 164.11K   404KB/s    in 0.4s    \n",
            "\n",
            "2022-04-07 03:07:22 (404 KB/s) - ‘cora.tgz.2’ saved [168052/168052]\n",
            "\n",
            "cora/\n",
            "cora/README\n",
            "cora/cora.cites\n",
            "cora/cora.content\n"
          ]
        }
      ],
      "source": [
        "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "! tar -zxvf cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYzURA4OKg"
      },
      "source": [
        "# import modules and set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uJQYMX02_z0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "seed = 0\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOv1h7YsK-5"
      },
      "source": [
        "# Loading and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kXPHN61i9keB"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    # Random indexes\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    # Nodes for training\n",
        "    idx_train = idx_rand[:training_samples]\n",
        "    # Nodes for validation\n",
        "    idx_val= idx_rand[training_samples:]\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"symmetric normalization\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzCZVd1JsbHr"
      },
      "source": [
        "## check the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KlsKjMKx8_b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc71505-a9cc-46e3-86ee-3738781b357b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "adj, features, labels, idx_train, idx_val = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mxrv21rLnpiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345c20e5-d08b-4810-eefd-92a4924b4fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
            "torch.Size([2708, 2708])\n"
          ]
        }
      ],
      "source": [
        "print(adj)\n",
        "print(adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWrDf0iWnpqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7381a7-cce4-46be-ae46-0b76376424c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([2708, 1433])\n"
          ]
        }
      ],
      "source": [
        "print(features)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TUkt2JJdsuA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a461767-9397-45f8-dc55-bf981078e2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "2708\n"
          ]
        }
      ],
      "source": [
        "print(labels)\n",
        "print(labels.unique())\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iGP18jNAs1Gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d8a44e-7896-41a6-f657-52e8e18562cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n",
            "2568\n"
          ]
        }
      ],
      "source": [
        "print(len(idx_train))\n",
        "print(len(idx_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqIcfH-vIic"
      },
      "source": [
        "# Vanilla GCN for node classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Graph Convolution layer (Your Task)\n",
        "\n",
        "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
        "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
        "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
      ],
      "metadata": {
        "id": "f48tylWyjLPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M-fU8L7f41VZ"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    A Graph Convolution Layer (GCN)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        \"\"\"\n",
        "        * `in_features`, $F$, is the number of input features per node\n",
        "        * `out_features`, $F'$, is the number of output features per node\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
        "        # hint: use nn.Linear()\n",
        "        ############ Your code here ###################################\n",
        "        self.w = nn.Linear(in_features, out_features)\n",
        "      \n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
        "        # to sum over neighbouring nodes )\n",
        "        # hint: use the linear layer you declared above. \n",
        "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
        "        #       adjacency matrix\n",
        "        ############ Your code here ###################################\n",
        "\n",
        "        output = torch.spmm(adj, self.w(input))\n",
        "        return output\n",
        "\n",
        "        ###############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GCN (Your Task)\n",
        "\n",
        "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
      ],
      "metadata": {
        "id": "RxBELCxkjF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    '''\n",
        "    A two-layer GCN\n",
        "    '''\n",
        "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
        "        \"\"\"\n",
        "        * `nfeat`, is the number of input features per node of the first layer\n",
        "        * `n_hidden`, number of hidden units\n",
        "        * `n_classes`, total number of classes for classification\n",
        "        * `dropout`, the dropout ratio\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "        # TODO: Initialization\n",
        "        # (1) 2 GraphConvolution() layers. \n",
        "        # (2) 1 Dropout layer\n",
        "        # (3) 1 activation function: ReLU()\n",
        "        ############ Your code here ###################################\n",
        "        self.gcn1 = GraphConvolution(nfeat, n_hidden)\n",
        "        self.gcn2 = GraphConvolution(n_hidden, n_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.af = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # TODO: the input will pass through the first graph convolution layer, \n",
        "        # the activation function, the dropout layer, then the second graph \n",
        "        # convolution layer. No activation function for the \n",
        "        # last layer. Return the logits. \n",
        "        ############ Your code here ###################################\n",
        "        x = self.gcn1(x, adj)\n",
        "        x = self.af(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gcn2(x, adj)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "        ###############################################################"
      ],
      "metadata": {
        "id": "HtVr2cN8jD5t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1d9F1G508r"
      },
      "source": [
        "## define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HyhqJ39OCzNN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXsdid6C5K1c"
      },
      "source": [
        "## training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bjlYeoFPFAWm"
      },
      "outputs": [],
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qbx0uc-9G5vs"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TjNiui83FYBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072a89f3-a7b8-4183-c2a5-3db83e2015a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GCN(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Vanilla GCN"
      ],
      "metadata": {
        "id": "1W6tqqj16iz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WSjUYJPSlnOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0746093f-dc6e-404d-9a19-90c6c157541c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9387 acc_train: 0.1143 loss_val: 1.9272 acc_val: 0.1597 time: 0.0165s\n",
            "Epoch: 0002 loss_train: 1.9319 acc_train: 0.1143 loss_val: 1.9223 acc_val: 0.1597 time: 0.0105s\n",
            "Epoch: 0003 loss_train: 1.9245 acc_train: 0.1143 loss_val: 1.9165 acc_val: 0.1663 time: 0.0099s\n",
            "Epoch: 0004 loss_train: 1.9154 acc_train: 0.1786 loss_val: 1.9103 acc_val: 0.2134 time: 0.0104s\n",
            "Epoch: 0005 loss_train: 1.9070 acc_train: 0.2000 loss_val: 1.9037 acc_val: 0.1340 time: 0.0117s\n",
            "Epoch: 0006 loss_train: 1.8966 acc_train: 0.2071 loss_val: 1.8968 acc_val: 0.1464 time: 0.0111s\n",
            "Epoch: 0007 loss_train: 1.8865 acc_train: 0.2429 loss_val: 1.8893 acc_val: 0.3470 time: 0.0112s\n",
            "Epoch: 0008 loss_train: 1.8768 acc_train: 0.4000 loss_val: 1.8815 acc_val: 0.3520 time: 0.0115s\n",
            "Epoch: 0009 loss_train: 1.8699 acc_train: 0.3857 loss_val: 1.8735 acc_val: 0.3104 time: 0.0105s\n",
            "Epoch: 0010 loss_train: 1.8577 acc_train: 0.3786 loss_val: 1.8654 acc_val: 0.3018 time: 0.0118s\n",
            "Epoch: 0011 loss_train: 1.8397 acc_train: 0.3500 loss_val: 1.8571 acc_val: 0.3014 time: 0.0102s\n",
            "Epoch: 0012 loss_train: 1.8268 acc_train: 0.3429 loss_val: 1.8487 acc_val: 0.3014 time: 0.0110s\n",
            "Epoch: 0013 loss_train: 1.8119 acc_train: 0.3286 loss_val: 1.8403 acc_val: 0.3014 time: 0.0113s\n",
            "Epoch: 0014 loss_train: 1.8050 acc_train: 0.3286 loss_val: 1.8320 acc_val: 0.3010 time: 0.0083s\n",
            "Epoch: 0015 loss_train: 1.7883 acc_train: 0.3286 loss_val: 1.8240 acc_val: 0.3010 time: 0.0103s\n",
            "Epoch: 0016 loss_train: 1.7659 acc_train: 0.3214 loss_val: 1.8162 acc_val: 0.3010 time: 0.0106s\n",
            "Epoch: 0017 loss_train: 1.7582 acc_train: 0.3214 loss_val: 1.8088 acc_val: 0.3010 time: 0.0090s\n",
            "Epoch: 0018 loss_train: 1.7511 acc_train: 0.3214 loss_val: 1.8020 acc_val: 0.3010 time: 0.0099s\n",
            "Epoch: 0019 loss_train: 1.7432 acc_train: 0.3214 loss_val: 1.7958 acc_val: 0.3010 time: 0.0106s\n",
            "Epoch: 0020 loss_train: 1.7293 acc_train: 0.3286 loss_val: 1.7901 acc_val: 0.3010 time: 0.0113s\n",
            "Epoch: 0021 loss_train: 1.7062 acc_train: 0.3214 loss_val: 1.7852 acc_val: 0.3010 time: 0.0097s\n",
            "Epoch: 0022 loss_train: 1.6965 acc_train: 0.3214 loss_val: 1.7807 acc_val: 0.3010 time: 0.0100s\n",
            "Epoch: 0023 loss_train: 1.7003 acc_train: 0.3214 loss_val: 1.7764 acc_val: 0.3010 time: 0.0085s\n",
            "Epoch: 0024 loss_train: 1.6738 acc_train: 0.3214 loss_val: 1.7720 acc_val: 0.3010 time: 0.0109s\n",
            "Epoch: 0025 loss_train: 1.6729 acc_train: 0.3214 loss_val: 1.7672 acc_val: 0.3010 time: 0.0086s\n",
            "Epoch: 0026 loss_train: 1.6607 acc_train: 0.3214 loss_val: 1.7615 acc_val: 0.3010 time: 0.0093s\n",
            "Epoch: 0027 loss_train: 1.6592 acc_train: 0.3214 loss_val: 1.7550 acc_val: 0.3010 time: 0.0104s\n",
            "Epoch: 0028 loss_train: 1.6370 acc_train: 0.3286 loss_val: 1.7471 acc_val: 0.3010 time: 0.0104s\n",
            "Epoch: 0029 loss_train: 1.6376 acc_train: 0.3214 loss_val: 1.7383 acc_val: 0.3014 time: 0.0106s\n",
            "Epoch: 0030 loss_train: 1.6031 acc_train: 0.3357 loss_val: 1.7287 acc_val: 0.3014 time: 0.0104s\n",
            "Epoch: 0031 loss_train: 1.6168 acc_train: 0.3357 loss_val: 1.7183 acc_val: 0.3014 time: 0.0099s\n",
            "Epoch: 0032 loss_train: 1.5878 acc_train: 0.3214 loss_val: 1.7078 acc_val: 0.3014 time: 0.0121s\n",
            "Epoch: 0033 loss_train: 1.5753 acc_train: 0.3429 loss_val: 1.6968 acc_val: 0.3026 time: 0.0104s\n",
            "Epoch: 0034 loss_train: 1.5645 acc_train: 0.3500 loss_val: 1.6854 acc_val: 0.3030 time: 0.0101s\n",
            "Epoch: 0035 loss_train: 1.5452 acc_train: 0.3571 loss_val: 1.6742 acc_val: 0.3061 time: 0.0107s\n",
            "Epoch: 0036 loss_train: 1.5276 acc_train: 0.3929 loss_val: 1.6630 acc_val: 0.3135 time: 0.0086s\n",
            "Epoch: 0037 loss_train: 1.5100 acc_train: 0.3857 loss_val: 1.6517 acc_val: 0.3217 time: 0.0099s\n",
            "Epoch: 0038 loss_train: 1.5060 acc_train: 0.4214 loss_val: 1.6402 acc_val: 0.3322 time: 0.0140s\n",
            "Epoch: 0039 loss_train: 1.4978 acc_train: 0.4714 loss_val: 1.6287 acc_val: 0.3450 time: 0.0100s\n",
            "Epoch: 0040 loss_train: 1.4586 acc_train: 0.4714 loss_val: 1.6172 acc_val: 0.3586 time: 0.0105s\n",
            "Epoch: 0041 loss_train: 1.4376 acc_train: 0.5286 loss_val: 1.6057 acc_val: 0.3676 time: 0.0104s\n",
            "Epoch: 0042 loss_train: 1.4544 acc_train: 0.4714 loss_val: 1.5944 acc_val: 0.3816 time: 0.0103s\n",
            "Epoch: 0043 loss_train: 1.4297 acc_train: 0.5000 loss_val: 1.5831 acc_val: 0.3894 time: 0.0097s\n",
            "Epoch: 0044 loss_train: 1.4057 acc_train: 0.5429 loss_val: 1.5720 acc_val: 0.3984 time: 0.0105s\n",
            "Epoch: 0045 loss_train: 1.4023 acc_train: 0.4857 loss_val: 1.5613 acc_val: 0.4026 time: 0.0105s\n",
            "Epoch: 0046 loss_train: 1.3701 acc_train: 0.5214 loss_val: 1.5510 acc_val: 0.4065 time: 0.0095s\n",
            "Epoch: 0047 loss_train: 1.3503 acc_train: 0.5500 loss_val: 1.5409 acc_val: 0.4116 time: 0.0105s\n",
            "Epoch: 0048 loss_train: 1.3406 acc_train: 0.5571 loss_val: 1.5308 acc_val: 0.4178 time: 0.0097s\n",
            "Epoch: 0049 loss_train: 1.3308 acc_train: 0.5643 loss_val: 1.5209 acc_val: 0.4264 time: 0.0102s\n",
            "Epoch: 0050 loss_train: 1.3267 acc_train: 0.5357 loss_val: 1.5107 acc_val: 0.4350 time: 0.0105s\n",
            "Epoch: 0051 loss_train: 1.3009 acc_train: 0.5714 loss_val: 1.5009 acc_val: 0.4470 time: 0.0091s\n",
            "Epoch: 0052 loss_train: 1.3013 acc_train: 0.5429 loss_val: 1.4909 acc_val: 0.4579 time: 0.0105s\n",
            "Epoch: 0053 loss_train: 1.2545 acc_train: 0.5643 loss_val: 1.4805 acc_val: 0.4727 time: 0.0104s\n",
            "Epoch: 0054 loss_train: 1.2635 acc_train: 0.6000 loss_val: 1.4701 acc_val: 0.4817 time: 0.0107s\n",
            "Epoch: 0055 loss_train: 1.2263 acc_train: 0.6357 loss_val: 1.4597 acc_val: 0.4945 time: 0.0090s\n",
            "Epoch: 0056 loss_train: 1.2386 acc_train: 0.6214 loss_val: 1.4491 acc_val: 0.5000 time: 0.0142s\n",
            "Epoch: 0057 loss_train: 1.2255 acc_train: 0.5857 loss_val: 1.4378 acc_val: 0.5047 time: 0.0115s\n",
            "Epoch: 0058 loss_train: 1.1820 acc_train: 0.6500 loss_val: 1.4263 acc_val: 0.5082 time: 0.0120s\n",
            "Epoch: 0059 loss_train: 1.1844 acc_train: 0.6429 loss_val: 1.4146 acc_val: 0.5113 time: 0.0099s\n",
            "Epoch: 0060 loss_train: 1.1836 acc_train: 0.6286 loss_val: 1.4032 acc_val: 0.5148 time: 0.0092s\n",
            "Epoch: 0061 loss_train: 1.1650 acc_train: 0.6214 loss_val: 1.3924 acc_val: 0.5202 time: 0.0101s\n",
            "Epoch: 0062 loss_train: 1.1318 acc_train: 0.6571 loss_val: 1.3822 acc_val: 0.5249 time: 0.0104s\n",
            "Epoch: 0063 loss_train: 1.1342 acc_train: 0.6357 loss_val: 1.3718 acc_val: 0.5284 time: 0.0104s\n",
            "Epoch: 0064 loss_train: 1.1161 acc_train: 0.6429 loss_val: 1.3617 acc_val: 0.5319 time: 0.0104s\n",
            "Epoch: 0065 loss_train: 1.1280 acc_train: 0.6500 loss_val: 1.3519 acc_val: 0.5378 time: 0.0103s\n",
            "Epoch: 0066 loss_train: 1.0710 acc_train: 0.6857 loss_val: 1.3423 acc_val: 0.5456 time: 0.0105s\n",
            "Epoch: 0067 loss_train: 1.0775 acc_train: 0.6571 loss_val: 1.3326 acc_val: 0.5537 time: 0.0096s\n",
            "Epoch: 0068 loss_train: 1.0435 acc_train: 0.6857 loss_val: 1.3231 acc_val: 0.5592 time: 0.0104s\n",
            "Epoch: 0069 loss_train: 1.0277 acc_train: 0.6929 loss_val: 1.3136 acc_val: 0.5646 time: 0.0101s\n",
            "Epoch: 0070 loss_train: 1.0202 acc_train: 0.6714 loss_val: 1.3041 acc_val: 0.5674 time: 0.0104s\n",
            "Epoch: 0071 loss_train: 1.0473 acc_train: 0.6857 loss_val: 1.2939 acc_val: 0.5717 time: 0.0094s\n",
            "Epoch: 0072 loss_train: 0.9992 acc_train: 0.7214 loss_val: 1.2845 acc_val: 0.5748 time: 0.0124s\n",
            "Epoch: 0073 loss_train: 0.9883 acc_train: 0.7286 loss_val: 1.2754 acc_val: 0.5767 time: 0.0147s\n",
            "Epoch: 0074 loss_train: 0.9673 acc_train: 0.7214 loss_val: 1.2660 acc_val: 0.5775 time: 0.0105s\n",
            "Epoch: 0075 loss_train: 0.9550 acc_train: 0.7357 loss_val: 1.2560 acc_val: 0.5810 time: 0.0128s\n",
            "Epoch: 0076 loss_train: 0.9518 acc_train: 0.7143 loss_val: 1.2468 acc_val: 0.5837 time: 0.0106s\n",
            "Epoch: 0077 loss_train: 0.9441 acc_train: 0.7357 loss_val: 1.2378 acc_val: 0.5872 time: 0.0105s\n",
            "Epoch: 0078 loss_train: 0.9319 acc_train: 0.7571 loss_val: 1.2292 acc_val: 0.5892 time: 0.0084s\n",
            "Epoch: 0079 loss_train: 0.9036 acc_train: 0.7643 loss_val: 1.2215 acc_val: 0.5919 time: 0.0104s\n",
            "Epoch: 0080 loss_train: 0.9106 acc_train: 0.7429 loss_val: 1.2132 acc_val: 0.5970 time: 0.0089s\n",
            "Epoch: 0081 loss_train: 0.9149 acc_train: 0.7429 loss_val: 1.2047 acc_val: 0.6001 time: 0.0102s\n",
            "Epoch: 0082 loss_train: 0.8926 acc_train: 0.7714 loss_val: 1.1965 acc_val: 0.6024 time: 0.0096s\n",
            "Epoch: 0083 loss_train: 0.9165 acc_train: 0.7500 loss_val: 1.1878 acc_val: 0.6063 time: 0.0078s\n",
            "Epoch: 0084 loss_train: 0.8798 acc_train: 0.7714 loss_val: 1.1790 acc_val: 0.6079 time: 0.0107s\n",
            "Epoch: 0085 loss_train: 0.8747 acc_train: 0.7786 loss_val: 1.1702 acc_val: 0.6098 time: 0.0105s\n",
            "Epoch: 0086 loss_train: 0.8627 acc_train: 0.7929 loss_val: 1.1616 acc_val: 0.6133 time: 0.0090s\n",
            "Epoch: 0087 loss_train: 0.8612 acc_train: 0.7929 loss_val: 1.1537 acc_val: 0.6153 time: 0.0101s\n",
            "Epoch: 0088 loss_train: 0.8698 acc_train: 0.8071 loss_val: 1.1461 acc_val: 0.6160 time: 0.0094s\n",
            "Epoch: 0089 loss_train: 0.8098 acc_train: 0.8357 loss_val: 1.1387 acc_val: 0.6195 time: 0.0099s\n",
            "Epoch: 0090 loss_train: 0.8286 acc_train: 0.8286 loss_val: 1.1313 acc_val: 0.6223 time: 0.0088s\n",
            "Epoch: 0091 loss_train: 0.7919 acc_train: 0.8286 loss_val: 1.1238 acc_val: 0.6254 time: 0.0095s\n",
            "Epoch: 0092 loss_train: 0.7797 acc_train: 0.8429 loss_val: 1.1160 acc_val: 0.6293 time: 0.0098s\n",
            "Epoch: 0093 loss_train: 0.7539 acc_train: 0.8786 loss_val: 1.1086 acc_val: 0.6336 time: 0.0101s\n",
            "Epoch: 0094 loss_train: 0.8075 acc_train: 0.8286 loss_val: 1.1020 acc_val: 0.6386 time: 0.0150s\n",
            "Epoch: 0095 loss_train: 0.7681 acc_train: 0.8500 loss_val: 1.0956 acc_val: 0.6421 time: 0.0095s\n",
            "Epoch: 0096 loss_train: 0.7314 acc_train: 0.8286 loss_val: 1.0892 acc_val: 0.6452 time: 0.0101s\n",
            "Epoch: 0097 loss_train: 0.7305 acc_train: 0.8786 loss_val: 1.0826 acc_val: 0.6491 time: 0.0091s\n",
            "Epoch: 0098 loss_train: 0.7108 acc_train: 0.8786 loss_val: 1.0760 acc_val: 0.6515 time: 0.0105s\n",
            "Epoch: 0099 loss_train: 0.7099 acc_train: 0.8714 loss_val: 1.0703 acc_val: 0.6488 time: 0.0102s\n",
            "Epoch: 0100 loss_train: 0.7307 acc_train: 0.8286 loss_val: 1.0643 acc_val: 0.6515 time: 0.0093s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.1258s\n",
            "Test set results: loss= 1.0643 accuracy= 0.6515\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# evaluating\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZF3eM6DhHfE_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XCFwzVLmPXnH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks"
      ],
      "metadata": {
        "id": "mKHEyXp1EVdo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx15HdotKnt_"
      },
      "source": [
        "## Graph attention layer (Your task)\n",
        "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
        "\n",
        "\n",
        "### The initial transformation\n",
        "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
        "\n",
        "### attention score\n",
        "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
        "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
        "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
        "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
        "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
        "\n",
        "#### How to vectorize this? Some hints: \n",
        "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
        "\n",
        "2. `tensor.repeat_interleave()` gives you\n",
        "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
        "\n",
        "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
        "\n",
        "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
        "\n",
        "\n",
        "#### Perform softmax \n",
        "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
        "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
        "\n",
        "#### Apply dropout\n",
        "Apply the dropout layer. (this step is easy)\n",
        "\n",
        "#### Calculate final output for each head\n",
        "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
        "\n",
        "\n",
        "#### Concat or Mean\n",
        "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wVu7rcOuAUZz"
      },
      "outputs": [],
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
        "                 is_concat: bool = True,\n",
        "                 dropout: float = 0.6,\n",
        "                 alpha: float = 0.2):\n",
        "        \"\"\"\n",
        "        in_features: F, the number of input features per node\n",
        "        out_features: F', the number of output features per node\n",
        "        n_heads: K, the number of attention heads\n",
        "        is_concat: whether the multi-head results should be concatenated or averaged\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative slope for leaky relu activation\n",
        "        \"\"\"\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        if is_concat:\n",
        "            assert out_features % n_heads == 0\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else:\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        # TODO: initialize the following modules: \n",
        "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
        "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
        "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
        "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
        "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
        "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
        "        ################ your code here ########################\n",
        "        self.W = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
        "        self.attention = nn.Linear(2 * self.n_hidden, 1, bias = False)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=alpha)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        ########################################################\n",
        "\n",
        "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        # Number of nodes\n",
        "        n_nodes = h.shape[0]\n",
        "        \n",
        "        # TODO: \n",
        "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
        "        #     (you can use tensor.view() function)\n",
        "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
        "        # (3) apply the attention layer \n",
        "        # (4) apply the activation layer (you will get the attention score e)\n",
        "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
        "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
        "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
        "        # (7) apply softmax \n",
        "        # (8) apply dropout_layer \n",
        "        ############## Your code here #########################################\n",
        "\n",
        "        s = self.W(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
        "        s_r = s.repeat(n_nodes, 1, 1)\n",
        "        s_ri = s.repeat_interleave(n_nodes, dim=0)\n",
        "        s_concat = torch.cat([s_ri, s_r], dim=-1).view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
        "        e = self.activation(self.attention(s_concat))\n",
        "        e = e.squeeze(-1)\n",
        "        # print(adj_mat.shape)\n",
        "        # print(e.shape)\n",
        "        adj_mat = torch.unsqueeze(adj_mat, 2)\n",
        "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
        "        a = self.dropout(self.softmax(e))\n",
        "\n",
        "      \n",
        "\n",
        "        #######################################################################\n",
        "\n",
        "        # Summation \n",
        "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
        "\n",
        "\n",
        "        # TODO: Concat or Mean\n",
        "        # Concatenate the heads\n",
        "        if self.is_concat:\n",
        "            ############## Your code here #########################################\n",
        "            return h_prime.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
        "\n",
        "\n",
        "            #######################################################################\n",
        "        # Take the mean of the heads (for the last layer)\n",
        "        else:\n",
        "            ############## Your code here #########################################\n",
        "            return h_prime.mean(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "            #######################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GAT network\n",
        "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
      ],
      "metadata": {
        "id": "YOSk_ZShi2nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
        "        \"\"\"\n",
        "        in_features: the number of features per node\n",
        "        n_hidden: the number of features in the first graph attention layer\n",
        "        n_classes: the number of classes\n",
        "        n_heads: the number of heads in the graph attention layers\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First graph attention layer where we concatenate the heads\n",
        "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
        "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
        "        self.activation = nn.ELU()  \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: the features vectors\n",
        "        adj_mat: the adjacency matrix\n",
        "        \"\"\"\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc1(x, adj_mat)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc2(x, adj_mat)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKNbUtPVi1Vs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training GAT"
      ],
      "metadata": {
        "id": "CtRQ3Ced7RAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        \"alpha\": 0.2,\n",
        "        \"n_heads\": 8\n",
        "        }"
      ],
      "metadata": {
        "id": "b7D5mYXC6zTG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7MYaK98hDy7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dcf1da-0b4e-4bc8-db03-f891eca1e0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GAT(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"],\n",
        "            alpha=args[\"alpha\"],\n",
        "            n_heads=args[\"n_heads\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "E9FcfXwMDzEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331251e9-d6df-4dcd-a07e-55951508c414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0001 loss_train: 1.9458 acc_train: 0.1714 loss_val: 1.9425 acc_val: 0.3976 time: 0.9562s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0002 loss_train: 1.9410 acc_train: 0.4000 loss_val: 1.9389 acc_val: 0.5043 time: 0.8411s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0003 loss_train: 1.9369 acc_train: 0.4786 loss_val: 1.9350 acc_val: 0.5456 time: 0.8357s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0004 loss_train: 1.9295 acc_train: 0.6143 loss_val: 1.9307 acc_val: 0.5444 time: 0.8417s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0005 loss_train: 1.9252 acc_train: 0.5357 loss_val: 1.9260 acc_val: 0.5319 time: 0.8325s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0006 loss_train: 1.9174 acc_train: 0.5000 loss_val: 1.9210 acc_val: 0.5273 time: 0.8483s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0007 loss_train: 1.9096 acc_train: 0.5929 loss_val: 1.9155 acc_val: 0.5132 time: 0.8336s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0008 loss_train: 1.8992 acc_train: 0.5500 loss_val: 1.9095 acc_val: 0.5012 time: 0.8397s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0009 loss_train: 1.8986 acc_train: 0.5643 loss_val: 1.9032 acc_val: 0.4930 time: 0.8372s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0010 loss_train: 1.8885 acc_train: 0.5429 loss_val: 1.8965 acc_val: 0.4836 time: 0.8426s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0011 loss_train: 1.8772 acc_train: 0.5214 loss_val: 1.8893 acc_val: 0.4747 time: 0.8320s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0012 loss_train: 1.8693 acc_train: 0.4929 loss_val: 1.8817 acc_val: 0.4673 time: 0.8476s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0013 loss_train: 1.8556 acc_train: 0.5571 loss_val: 1.8737 acc_val: 0.4564 time: 0.8337s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0014 loss_train: 1.8450 acc_train: 0.5000 loss_val: 1.8654 acc_val: 0.4521 time: 0.8435s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0015 loss_train: 1.8446 acc_train: 0.4929 loss_val: 1.8567 acc_val: 0.4482 time: 0.8368s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0016 loss_train: 1.8232 acc_train: 0.4643 loss_val: 1.8476 acc_val: 0.4404 time: 0.8425s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0017 loss_train: 1.8098 acc_train: 0.4929 loss_val: 1.8381 acc_val: 0.4389 time: 0.8335s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0018 loss_train: 1.7938 acc_train: 0.5286 loss_val: 1.8282 acc_val: 0.4365 time: 0.8479s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0019 loss_train: 1.7994 acc_train: 0.5357 loss_val: 1.8181 acc_val: 0.4369 time: 0.8344s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0020 loss_train: 1.7830 acc_train: 0.4714 loss_val: 1.8078 acc_val: 0.4369 time: 0.8401s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0021 loss_train: 1.7594 acc_train: 0.4500 loss_val: 1.7973 acc_val: 0.4377 time: 0.8368s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0022 loss_train: 1.7530 acc_train: 0.4429 loss_val: 1.7865 acc_val: 0.4389 time: 0.8430s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0023 loss_train: 1.7222 acc_train: 0.4857 loss_val: 1.7753 acc_val: 0.4393 time: 0.8316s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0024 loss_train: 1.7137 acc_train: 0.4643 loss_val: 1.7639 acc_val: 0.4420 time: 0.8494s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0025 loss_train: 1.7129 acc_train: 0.4929 loss_val: 1.7523 acc_val: 0.4439 time: 0.8341s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0026 loss_train: 1.6787 acc_train: 0.4500 loss_val: 1.7405 acc_val: 0.4474 time: 0.8419s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0027 loss_train: 1.6548 acc_train: 0.4929 loss_val: 1.7285 acc_val: 0.4509 time: 0.8371s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0028 loss_train: 1.6519 acc_train: 0.4643 loss_val: 1.7163 acc_val: 0.4544 time: 0.8420s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0029 loss_train: 1.6519 acc_train: 0.4643 loss_val: 1.7041 acc_val: 0.4591 time: 0.8318s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0030 loss_train: 1.5910 acc_train: 0.5000 loss_val: 1.6917 acc_val: 0.4630 time: 0.8500s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0031 loss_train: 1.6152 acc_train: 0.4714 loss_val: 1.6792 acc_val: 0.4665 time: 0.8352s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0032 loss_train: 1.5987 acc_train: 0.4929 loss_val: 1.6666 acc_val: 0.4712 time: 0.8426s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0033 loss_train: 1.5913 acc_train: 0.4571 loss_val: 1.6539 acc_val: 0.4755 time: 0.8341s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0034 loss_train: 1.6405 acc_train: 0.4357 loss_val: 1.6415 acc_val: 0.4801 time: 0.8411s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0035 loss_train: 1.5782 acc_train: 0.4929 loss_val: 1.6292 acc_val: 0.4844 time: 0.8310s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0036 loss_train: 1.4697 acc_train: 0.5357 loss_val: 1.6166 acc_val: 0.4903 time: 0.8477s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0037 loss_train: 1.5738 acc_train: 0.5214 loss_val: 1.6042 acc_val: 0.4992 time: 0.8328s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0038 loss_train: 1.4913 acc_train: 0.6071 loss_val: 1.5917 acc_val: 0.5051 time: 0.8432s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0039 loss_train: 1.4528 acc_train: 0.5429 loss_val: 1.5793 acc_val: 0.5093 time: 0.8377s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0040 loss_train: 1.4937 acc_train: 0.6143 loss_val: 1.5670 acc_val: 0.5156 time: 0.8433s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0041 loss_train: 1.4954 acc_train: 0.5643 loss_val: 1.5550 acc_val: 0.5257 time: 0.8333s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0042 loss_train: 1.4620 acc_train: 0.5500 loss_val: 1.5430 acc_val: 0.5323 time: 0.8482s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0043 loss_train: 1.3971 acc_train: 0.6143 loss_val: 1.5310 acc_val: 0.5409 time: 0.8331s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0044 loss_train: 1.3538 acc_train: 0.6286 loss_val: 1.5189 acc_val: 0.5518 time: 0.8424s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0045 loss_train: 1.4581 acc_train: 0.6000 loss_val: 1.5070 acc_val: 0.5631 time: 0.8332s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0046 loss_train: 1.3647 acc_train: 0.6571 loss_val: 1.4952 acc_val: 0.5697 time: 0.8429s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0047 loss_train: 1.3407 acc_train: 0.6429 loss_val: 1.4834 acc_val: 0.5864 time: 0.8334s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0048 loss_train: 1.3162 acc_train: 0.6571 loss_val: 1.4717 acc_val: 0.6020 time: 0.8429s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0049 loss_train: 1.3606 acc_train: 0.6500 loss_val: 1.4602 acc_val: 0.6133 time: 0.8342s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0050 loss_train: 1.3157 acc_train: 0.6929 loss_val: 1.4489 acc_val: 0.6258 time: 0.8458s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0051 loss_train: 1.2396 acc_train: 0.7286 loss_val: 1.4377 acc_val: 0.6367 time: 0.8331s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0052 loss_train: 1.2517 acc_train: 0.6929 loss_val: 1.4266 acc_val: 0.6503 time: 0.8450s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0053 loss_train: 1.2751 acc_train: 0.7143 loss_val: 1.4157 acc_val: 0.6581 time: 0.8338s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0054 loss_train: 1.3055 acc_train: 0.6857 loss_val: 1.4052 acc_val: 0.6725 time: 0.8433s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0055 loss_train: 1.2254 acc_train: 0.7214 loss_val: 1.3947 acc_val: 0.6885 time: 0.8331s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0056 loss_train: 1.2076 acc_train: 0.7500 loss_val: 1.3841 acc_val: 0.6974 time: 0.8468s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0057 loss_train: 1.2046 acc_train: 0.7714 loss_val: 1.3738 acc_val: 0.7087 time: 0.8442s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0058 loss_train: 1.2113 acc_train: 0.7571 loss_val: 1.3634 acc_val: 0.7169 time: 0.8540s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0059 loss_train: 1.1702 acc_train: 0.8214 loss_val: 1.3531 acc_val: 0.7231 time: 0.8351s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0060 loss_train: 1.2497 acc_train: 0.7571 loss_val: 1.3431 acc_val: 0.7325 time: 0.8430s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0061 loss_train: 1.1656 acc_train: 0.8071 loss_val: 1.3335 acc_val: 0.7407 time: 0.8360s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0062 loss_train: 1.1535 acc_train: 0.8000 loss_val: 1.3240 acc_val: 0.7477 time: 0.8450s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0063 loss_train: 1.2209 acc_train: 0.7643 loss_val: 1.3147 acc_val: 0.7488 time: 0.8352s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0064 loss_train: 1.1363 acc_train: 0.7643 loss_val: 1.3055 acc_val: 0.7523 time: 0.8453s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0065 loss_train: 1.0838 acc_train: 0.8143 loss_val: 1.2964 acc_val: 0.7578 time: 0.8326s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0066 loss_train: 1.1560 acc_train: 0.7714 loss_val: 1.2876 acc_val: 0.7582 time: 0.8439s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0067 loss_train: 1.1255 acc_train: 0.7857 loss_val: 1.2793 acc_val: 0.7593 time: 0.8356s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0068 loss_train: 1.0971 acc_train: 0.7500 loss_val: 1.2716 acc_val: 0.7632 time: 0.8449s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0069 loss_train: 1.0251 acc_train: 0.8500 loss_val: 1.2637 acc_val: 0.7656 time: 0.8351s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0070 loss_train: 1.1534 acc_train: 0.7857 loss_val: 1.2560 acc_val: 0.7699 time: 0.8459s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0071 loss_train: 1.0610 acc_train: 0.8857 loss_val: 1.2483 acc_val: 0.7718 time: 0.8337s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0072 loss_train: 1.1323 acc_train: 0.7643 loss_val: 1.2407 acc_val: 0.7741 time: 0.8425s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0073 loss_train: 1.0556 acc_train: 0.8500 loss_val: 1.2328 acc_val: 0.7745 time: 0.8374s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0074 loss_train: 1.1348 acc_train: 0.7929 loss_val: 1.2247 acc_val: 0.7734 time: 0.8451s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0075 loss_train: 1.0670 acc_train: 0.8286 loss_val: 1.2165 acc_val: 0.7738 time: 0.8357s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0076 loss_train: 1.0130 acc_train: 0.8071 loss_val: 1.2084 acc_val: 0.7753 time: 0.8482s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0077 loss_train: 0.9913 acc_train: 0.8500 loss_val: 1.2002 acc_val: 0.7749 time: 0.8351s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0078 loss_train: 1.0103 acc_train: 0.8286 loss_val: 1.1918 acc_val: 0.7757 time: 0.8432s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0079 loss_train: 1.0097 acc_train: 0.8571 loss_val: 1.1832 acc_val: 0.7749 time: 0.8355s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0080 loss_train: 1.0831 acc_train: 0.7714 loss_val: 1.1746 acc_val: 0.7749 time: 0.8440s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0081 loss_train: 1.0386 acc_train: 0.8071 loss_val: 1.1660 acc_val: 0.7738 time: 0.8343s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0082 loss_train: 0.9251 acc_train: 0.8429 loss_val: 1.1574 acc_val: 0.7749 time: 0.8450s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0083 loss_train: 1.0942 acc_train: 0.7357 loss_val: 1.1489 acc_val: 0.7749 time: 0.8347s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0084 loss_train: 0.9746 acc_train: 0.8143 loss_val: 1.1405 acc_val: 0.7734 time: 0.8461s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0085 loss_train: 1.0808 acc_train: 0.8071 loss_val: 1.1322 acc_val: 0.7726 time: 0.8354s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0086 loss_train: 0.9708 acc_train: 0.7643 loss_val: 1.1242 acc_val: 0.7722 time: 0.8470s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0087 loss_train: 0.9395 acc_train: 0.8571 loss_val: 1.1163 acc_val: 0.7722 time: 0.8365s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0088 loss_train: 0.9157 acc_train: 0.7857 loss_val: 1.1087 acc_val: 0.7726 time: 0.8509s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0089 loss_train: 0.9528 acc_train: 0.8214 loss_val: 1.1012 acc_val: 0.7734 time: 0.8340s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0090 loss_train: 0.9987 acc_train: 0.7786 loss_val: 1.0940 acc_val: 0.7741 time: 0.8472s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0091 loss_train: 0.9005 acc_train: 0.8500 loss_val: 1.0868 acc_val: 0.7749 time: 0.8369s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0092 loss_train: 0.8740 acc_train: 0.8143 loss_val: 1.0799 acc_val: 0.7757 time: 0.8500s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0093 loss_train: 0.8831 acc_train: 0.8286 loss_val: 1.0732 acc_val: 0.7757 time: 0.8373s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0094 loss_train: 0.8885 acc_train: 0.8286 loss_val: 1.0666 acc_val: 0.7749 time: 0.8482s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0095 loss_train: 0.8396 acc_train: 0.8286 loss_val: 1.0602 acc_val: 0.7761 time: 0.8365s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0096 loss_train: 0.8975 acc_train: 0.8143 loss_val: 1.0539 acc_val: 0.7765 time: 0.8484s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0097 loss_train: 0.8978 acc_train: 0.7857 loss_val: 1.0478 acc_val: 0.7788 time: 0.8396s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0098 loss_train: 0.9480 acc_train: 0.7786 loss_val: 1.0417 acc_val: 0.7804 time: 0.8487s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0099 loss_train: 1.0023 acc_train: 0.7643 loss_val: 1.0357 acc_val: 0.7819 time: 0.8389s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Epoch: 0100 loss_train: 0.8986 acc_train: 0.8214 loss_val: 1.0298 acc_val: 0.7839 time: 0.8502s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 84.2320s\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 8])\n",
            "torch.Size([2708, 2708])\n",
            "torch.Size([2708, 2708, 1])\n",
            "Test set results: loss= 1.0298 accuracy= 0.7839\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question: (Your task)\n",
        "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
      ],
      "metadata": {
        "id": "n6Ox3fbTG7rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:\n",
        "GAT yields a better test set result than GCN, where GAT has loss= 1.0298 accuracy= 0.7839 and GCN has loss= 1.0643 accuracy= 0.6515. GAT uses self attention to learn the relative importance of each neighbouring node, whereas GCN aggregates the normalized sum of neighboring information. Hence GAT yields a better result since the model learns the meaningful parts in neighbouring nodes."
      ],
      "metadata": {
        "id": "GHboFUy5C6LT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of a4_GCN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}